{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOUqzu/nbqlvZ1akpMovY20",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ah-ma-algs/image-2-image-using-diffusers/blob/main/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slAR0WxveALp",
        "outputId": "70209430-6232-4437-edcd-3440de065ed3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬ Downloading https://github.com/jaimergp/miniforge/releases/download/24.11.2-1_colab/Miniforge3-colab-24.11.2-1_colab-Linux-x86_64.sh...\n",
            "ðŸ“¦ Installing...\n",
            "ðŸ“Œ Adjusting configuration...\n",
            "ðŸ©¹ Patching environment...\n",
            "â² Done in 0:00:16\n",
            "ðŸ” Restarting kernel...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4R1ZD6TdanA",
        "outputId": "6361e8e5-ae12-4565-aff0-c53f9fe5e0b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Looking for: ['python=3.10']\n",
            "\n",
            "\u001b[?25l\u001b[2K\u001b[0G[+] 0.0s\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.1s\n",
            "conda-forge/linux-64  â£¾  \n",
            "conda-forge/noarch    â£¾  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.2s\n",
            "conda-forge/linux-64   1%\n",
            "conda-forge/noarch     1%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.3s\n",
            "conda-forge/linux-64   6%\n",
            "conda-forge/noarch     9%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.4s\n",
            "conda-forge/linux-64   9%\n",
            "conda-forge/noarch    22%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.5s\n",
            "conda-forge/linux-64  14%\n",
            "conda-forge/noarch    30%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.6s\n",
            "conda-forge/linux-64  19%\n",
            "conda-forge/noarch    41%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.7s\n",
            "conda-forge/linux-64  23%\n",
            "conda-forge/noarch    50%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.8s\n",
            "conda-forge/linux-64  26%\n",
            "conda-forge/noarch    57%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.9s\n",
            "conda-forge/linux-64  29%\n",
            "conda-forge/noarch    60%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.0s\n",
            "conda-forge/linux-64  30%\n",
            "conda-forge/noarch    68%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.1s\n",
            "conda-forge/linux-64  30%\n",
            "conda-forge/noarch    68%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.2s\n",
            "conda-forge/linux-64  32%\n",
            "conda-forge/noarch    71%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.3s\n",
            "conda-forge/linux-64  33%\n",
            "conda-forge/noarch    73%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.4s\n",
            "conda-forge/linux-64  34%\n",
            "conda-forge/noarch    76%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.5s\n",
            "conda-forge/linux-64  35%\n",
            "conda-forge/noarch    78%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.6s\n",
            "conda-forge/linux-64  36%\n",
            "conda-forge/noarch    79%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.7s\n",
            "conda-forge/linux-64  37%\n",
            "conda-forge/noarch    81%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.8s\n",
            "conda-forge/linux-64  38%\n",
            "conda-forge/noarch    83%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.9s\n",
            "conda-forge/linux-64  40%\n",
            "conda-forge/noarch    86%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.0s\n",
            "conda-forge/linux-64  41%\n",
            "conda-forge/noarch    90%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.1s\n",
            "conda-forge/linux-64  41%\n",
            "conda-forge/noarch    92%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.2s\n",
            "conda-forge/linux-64  42%\n",
            "conda-forge/noarch    92%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.3s\n",
            "conda-forge/linux-64  42%\n",
            "conda-forge/noarch    92%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.4s\n",
            "conda-forge/linux-64  42%\n",
            "conda-forge/noarch    92%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.5s\n",
            "conda-forge/linux-64  42%\n",
            "conda-forge/noarch    92%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.6s\n",
            "conda-forge/linux-64  42%\n",
            "conda-forge/noarch    92%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.7s\n",
            "conda-forge/linux-64  43%\n",
            "conda-forge/noarch    92%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.8s\n",
            "conda-forge/linux-64  43%\n",
            "conda-forge/noarch    92%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 2.9s\n",
            "conda-forge/linux-64  43%\n",
            "conda-forge/noarch    92%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 3.0s\n",
            "conda-forge/linux-64  43%\n",
            "conda-forge/noarch    92%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 3.1s\n",
            "conda-forge/linux-64  43%\n",
            "conda-forge/noarch    93%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 3.2s\n",
            "conda-forge/linux-64  44%\n",
            "conda-forge/noarch    99%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gconda-forge/noarch                                \n",
            "[+] 3.3s\n",
            "conda-forge/linux-64  47%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 3.4s\n",
            "conda-forge/linux-64  53%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 3.5s\n",
            "conda-forge/linux-64  54%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 3.6s\n",
            "conda-forge/linux-64  56%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 3.7s\n",
            "conda-forge/linux-64  58%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 3.8s\n",
            "conda-forge/linux-64  59%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 3.9s\n",
            "conda-forge/linux-64  59%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 4.0s\n",
            "conda-forge/linux-64  60%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 4.1s\n",
            "conda-forge/linux-64  62%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 4.2s\n",
            "conda-forge/linux-64  67%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 4.3s\n",
            "conda-forge/linux-64  73%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 4.4s\n",
            "conda-forge/linux-64  73%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 4.5s\n",
            "conda-forge/linux-64  77%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 4.6s\n",
            "conda-forge/linux-64  83%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 4.7s\n",
            "conda-forge/linux-64  90%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 4.8s\n",
            "conda-forge/linux-64  97%\u001b[2K\u001b[1A\u001b[2K\u001b[0Gconda-forge/linux-64                              \n",
            "\u001b[?25hTransaction\n",
            "\n",
            "  Prefix: /usr/local/envs/icontrol3d\n",
            "\n",
            "  Updating specs:\n",
            "\n",
            "   - python=3.10\n",
            "\n",
            "\n",
            "  Package               Version  Build               Channel           Size\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "  Install:\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "  \u001b[32m+ ld_impl_linux-64\u001b[0m       2.43  h712a8e2_4          conda-forge\u001b[32m     Cached\u001b[0m\n",
            "  \u001b[32m+ ca-certificates \u001b[0m  2025.1.31  hbcca054_0          conda-forge\u001b[32m     Cached\u001b[0m\n",
            "  \u001b[32m+ _libgcc_mutex   \u001b[0m        0.1  conda_forge         conda-forge\u001b[32m     Cached\u001b[0m\n",
            "  \u001b[32m+ libgomp         \u001b[0m     14.2.0  h767d61c_2          conda-forge\u001b[32m     Cached\u001b[0m\n",
            "  \u001b[32m+ _openmp_mutex   \u001b[0m        4.5  2_gnu               conda-forge\u001b[32m     Cached\u001b[0m\n",
            "  \u001b[32m+ libgcc          \u001b[0m     14.2.0  h767d61c_2          conda-forge\u001b[32m     Cached\u001b[0m\n",
            "  \u001b[32m+ openssl         \u001b[0m      3.4.1  h7b32b05_0          conda-forge\u001b[32m     Cached\u001b[0m\n",
            "  \u001b[32m+ ncurses         \u001b[0m        6.5  h2d0b736_3          conda-forge\u001b[32m     Cached\u001b[0m\n",
            "  \u001b[32m+ libzlib         \u001b[0m      1.3.1  hb9d3cd8_2          conda-forge\u001b[32m     Cached\u001b[0m\n",
            "  \u001b[32m+ liblzma         \u001b[0m      5.6.4  hb9d3cd8_0          conda-forge\u001b[32m     Cached\u001b[0m\n",
            "  \u001b[32m+ libgcc-ng       \u001b[0m     14.2.0  h69a702a_2          conda-forge\u001b[32m     Cached\u001b[0m\n",
            "  \u001b[32m+ libffi          \u001b[0m      3.4.6  h2dba641_0          conda-forge\u001b[32m     Cached\u001b[0m\n",
            "  \u001b[32m+ readline        \u001b[0m        8.2  h8c095d6_2          conda-forge\u001b[32m     Cached\u001b[0m\n",
            "  \u001b[32m+ libsqlite       \u001b[0m     3.49.1  hee588c1_2          conda-forge\u001b[32m     Cached\u001b[0m\n",
            "  \u001b[32m+ tk              \u001b[0m     8.6.13  noxft_h4845f30_101  conda-forge\u001b[32m     Cached\u001b[0m\n",
            "  \u001b[32m+ libxcrypt       \u001b[0m     4.4.36  hd590300_1          conda-forge\u001b[32m     Cached\u001b[0m\n",
            "  \u001b[32m+ bzip2           \u001b[0m      1.0.8  h4bc722e_7          conda-forge\u001b[32m     Cached\u001b[0m\n",
            "  \u001b[32m+ libuuid         \u001b[0m     2.38.1  h0b41bf4_0          conda-forge\u001b[32m     Cached\u001b[0m\n",
            "  \u001b[32m+ libnsl          \u001b[0m      2.0.1  hd590300_0          conda-forge\u001b[32m     Cached\u001b[0m\n",
            "  \u001b[32m+ tzdata          \u001b[0m      2025a  h78e105d_0          conda-forge\u001b[32m     Cached\u001b[0m\n",
            "  \u001b[32m+ python          \u001b[0m    3.10.16  he725a3c_1_cpython  conda-forge\u001b[32m     Cached\u001b[0m\n",
            "  \u001b[32m+ wheel           \u001b[0m     0.45.1  pyhd8ed1ab_1        conda-forge\u001b[32m     Cached\u001b[0m\n",
            "  \u001b[32m+ setuptools      \u001b[0m     75.8.2  pyhff2d567_0        conda-forge\u001b[32m     Cached\u001b[0m\n",
            "  \u001b[32m+ pip             \u001b[0m     25.0.1  pyh8b19718_0        conda-forge\u001b[32m     Cached\u001b[0m\n",
            "\n",
            "  Summary:\n",
            "\n",
            "  Install: 24 packages\n",
            "\n",
            "  Total download: 0 B\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "\n",
            "\u001b[?25l\u001b[2K\u001b[0G\u001b[?25h\n",
            "Downloading and Extracting Packages:\n",
            "\n",
            "Preparing transaction: - \b\b\\ \b\b| \b\bdone\n",
            "Verifying transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Executing transaction: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "\n",
            "To activate this environment, use\n",
            "\n",
            "     $ mamba activate icontrol3d\n",
            "\n",
            "To deactivate an active environment, use\n",
            "\n",
            "     $ mamba deactivate\n",
            "\n",
            "Run 'mamba init' to be able to run mamba activate/deactivate\n",
            "and start a new shell session. Or use conda to activate/deactivate.\n",
            "\n",
            "\n",
            "Looking for: ['pytorch=1.13.0', 'torchvision', 'torchaudio', 'pytorch-cuda=11.6']\n",
            "\n",
            "conda-forge/linux-64                                        Using cache\n",
            "conda-forge/noarch                                          Using cache\n",
            "\u001b[?25l\u001b[2K\u001b[0G[+] 0.0s\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[0Gnvidia/linux-64                                               No change\n",
            "pytorch/noarch                                                No change\n",
            "[+] 0.1s\n",
            "pytorch/linux-64  â£¾  \n",
            "nvidia/noarch     â£¾  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gpytorch/linux-64                                              No change\n",
            "nvidia/noarch                                                 No change\n",
            "\u001b[?25h\n",
            "Pinned packages:\n",
            "  - python 3.11.*\n",
            "  - python 3.11.*\n",
            "  - python_abi 3.11.* *cp311*\n",
            "  - cuda-version 12.*\n",
            "\n",
            "\n",
            "\u001b[33m\u001b[1mwarning  libmamba\u001b[m Added empty dependency for problem type SOLVER_RULE_UPDATE\n",
            "Could not solve for environment specs\n",
            "The following packages are incompatible\n",
            "â”œâ”€ \u001b[32mlibtorch\u001b[0m is installable with the potential options\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.6.0\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.6.0 cpu_generic_*_0\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.1.0\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.1.0 cpu_generic_*_2\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.1.0\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.1.0 cpu_generic_*_3\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.1.0\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.1.0 cpu_mkl_*_102\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.1.0\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.1.0 cpu_mkl_*_103\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.1.0\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.1.0 cuda112_*_302\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.1.0\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.1.0 cuda112_*_303\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.1.0\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.1.0 cuda118_*_302\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.1.0\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.1.0 cuda118_*_303\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.1.0\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.1.0 cuda120_*_302\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.1.0\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.1.0 cuda120_*_303\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.1.2\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.1.2 cpu_generic_*_4\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.1.2\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.1.2 cpu_generic_*_0\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.1.2\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.1.2 cpu_generic_*_1\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.1.2\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.1.2 cpu_generic_*_3\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.1.2\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.1.2 cpu_mkl_*_100\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.1.2\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.1.2 cpu_mkl_*_101\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.1.2\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.1.2 cpu_mkl_*_103\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.1.2\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.1.2 cpu_mkl_*_104\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.1.2\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.1.2 cuda112_*_300\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.1.2\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.1.2 cuda112_*_301\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.1.2\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.1.2 cuda118_*_301\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.1.2\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.1.2 cuda118_*_303\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.1.2\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.1.2 cuda118_*_300\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.1.2\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.1.2 cuda118_*_304\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.1.2\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.1.2 cuda120_*_301\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.1.2\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.1.2 cuda120_*_303\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.1.2\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.1.2 cuda120_*_300\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.1.2\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.1.2 cuda120_*_304\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.3.0\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.3.0 cpu_generic_*_0\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.3.0\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.3.0 cpu_generic_*_1\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.3.0\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.3.0 cpu_mkl_*_101\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.3.0\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.3.0 cpu_mkl_*_100\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.3.0\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.3.0 cuda118_*_301\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.3.0\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.3.0 cuda118_*_300\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.3.0\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.3.0 cuda120_*_301\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.3.0\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.3.0 cuda120_*_300\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.3.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.3.1 cpu_generic_*_0\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.3.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.3.1 cpu_mkl_*_100\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.3.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.3.1 cuda118_*_300\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.3.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.3.1 cuda120_*_300\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.4.0\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.4.0 cpu_generic_*_1\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.4.0\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.4.0 cpu_generic_*_0\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.4.0\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.4.0 cpu_mkl_*_100\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.4.0\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.4.0 cpu_mkl_*_101\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.4.0\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.4.0 cuda118_*_300\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.4.0\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.4.0 cuda118_*_301\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.4.0\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.4.0 cuda120_*_300\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.4.0\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.4.0 cuda120_*_301\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.4.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.4.1 cpu_generic_*_3\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.4.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.4.1 cpu_generic_*_4\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.4.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.4.1 cpu_generic_*_0\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.4.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.4.1 cpu_generic_*_1\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.4.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.4.1 cpu_generic_*_2\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.4.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.4.1 cpu_generic_*_5\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.4.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.4.1 cpu_mkl_*_103\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.4.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.4.1 cpu_mkl_*_104\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.4.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.4.1 cpu_mkl_*_101\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.4.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.4.1 cpu_mkl_*_102\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.4.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.4.1 cpu_mkl_*_100\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.4.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.4.1 cpu_mkl_*_105\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.4.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.4.1 cuda118_*_303\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.4.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.4.1 cuda118_*_305\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.4.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.4.1 cuda118_*_304\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.4.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.4.1 cuda118_*_300\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.4.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.4.1 cuda118_*_301\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.4.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.4.1 cuda118_*_302\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.4.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.4.1 cuda120_*_301\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.4.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.4.1 cuda120_*_302\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.4.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.4.1 cuda120_*_305\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.4.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.4.1 cuda120_*_300\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.4.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.4.1 cuda120_*_303\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.4.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.4.1 cuda120_*_304\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.5.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.5.1 cpu_generic_*_3\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.5.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.5.1 cpu_generic_*_0\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.5.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.5.1 cpu_generic_*_2\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.5.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.5.1 cpu_generic_*_6\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.5.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.5.1 cpu_generic_*_7\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.5.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.5.1 cpu_generic_*_8\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.5.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.5.1 cpu_generic_*_15\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.5.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.5.1 cpu_generic_*_9\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.5.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.5.1 cpu_generic_*_10\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.5.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.5.1 cpu_generic_*_11\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.5.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.5.1 cpu_generic_*_13\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.5.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.5.1 cpu_generic_*_4\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.5.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.5.1 cpu_generic_*_5\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.5.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.5.1 cpu_generic_*_16\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.5.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.5.1 cpu_mkl_*_104\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.5.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.5.1 cpu_mkl_*_105\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.5.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.5.1 cpu_mkl_*_106\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.5.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.5.1 cpu_mkl_*_107\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.5.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.5.1 cpu_mkl_*_110\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.5.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.5.1 cpu_mkl_*_111\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.5.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.5.1 cpu_mkl_*_113\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.5.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.5.1 cpu_mkl_*_109\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.5.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.5.1 cpu_mkl_*_103\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.5.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.5.1 cpu_mkl_*_115\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.5.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.5.1 cpu_mkl_*_100\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.5.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.5.1 cpu_mkl_*_102\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.5.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.5.1 cpu_mkl_*_108\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.5.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.5.1 cpu_mkl_*_116\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.5.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.5.1 cuda118_*_303\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.5.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.5.1 cuda118_*_300\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.5.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.5.1 cuda118_*_302\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.5.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.5.1 cuda120_*_300\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.5.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.5.1 cuda120_*_302\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.5.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.5.1 cuda120_*_303\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.5.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.5.1 cuda126_generic_*_210\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.5.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.5.1 cuda126_generic_*_211\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.5.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.5.1 cuda126_generic_*_213\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.5.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.5.1 cuda126_generic_*_207\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.5.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.5.1 cuda126_generic_*_216\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.5.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.5.1 cuda126_generic_*_208\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.5.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.5.1 cuda126_generic_*_209\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.5.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.5.1 cuda126_generic_*_215\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.5.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.5.1 cuda126_*_301\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.5.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.5.1 cuda126_*_302\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.5.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.5.1 cuda126_*_206\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.5.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.5.1 cuda126_*_303\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.5.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.5.1 cuda126_*_204\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.5.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.5.1 cuda126_*_205\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.5.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.5.1 cuda126_*_306\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.5.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.5.1 cuda126_*_304\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.5.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.5.1 cuda126_*_305\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.5.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.5.1 cuda126_mkl_*_308\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.5.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.5.1 cuda126_mkl_*_315\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.5.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.5.1 cuda126_mkl_*_316\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.5.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.5.1 cuda126_mkl_*_310\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.5.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.5.1 cuda126_mkl_*_313\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.5.1\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.5.1 cuda126_mkl_*_309\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.6.0\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.6.0 cpu_generic_*_1\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.6.0\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.6.0 cpu_generic_*_2\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.6.0\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.6.0 cpu_mkl_*_100\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.6.0\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.6.0 cpu_mkl_*_101\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.6.0\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.6.0 cpu_mkl_*_102\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.6.0\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.6.0 cuda126_generic_*_200\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.6.0\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.6.0 cuda126_generic_*_202\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.6.0\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.6.0 cuda126_generic_*_201\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.6.0\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.6.0 cuda126_mkl_*_300\u001b[0m, which can be installed;\n",
            "â”‚  â”œâ”€ \u001b[32mlibtorch 2.6.0\u001b[0m would require\n",
            "â”‚  â”‚  â””â”€ \u001b[32mpytorch 2.6.0 cuda126_mkl_*_301\u001b[0m, which can be installed;\n",
            "â”‚  â””â”€ \u001b[32mlibtorch 2.6.0\u001b[0m would require\n",
            "â”‚     â””â”€ \u001b[32mpytorch 2.6.0 cuda126_mkl_*_302\u001b[0m, which can be installed;\n",
            "â””â”€ \u001b[32mpytorch 1.13.0** \u001b[0m is installable with the potential options\n",
            "   â”œâ”€ \u001b[31mpytorch 1.13.0\u001b[0m conflicts with any installable versions previously reported;\n",
            "   â”œâ”€ \u001b[32mpytorch 1.13.0\u001b[0m would require\n",
            "   â”‚  â””â”€ \u001b[32mpython >=3.10,<3.11.0a0 \u001b[0m, which can be installed;\n",
            "   â”œâ”€ \u001b[32mpytorch 1.13.0\u001b[0m would require\n",
            "   â”‚  â””â”€ \u001b[32mpython >=3.7,<3.8.0a0 \u001b[0m, which can be installed;\n",
            "   â”œâ”€ \u001b[32mpytorch 1.13.0\u001b[0m would require\n",
            "   â”‚  â””â”€ \u001b[32mpython >=3.8,<3.9.0a0 \u001b[0m, which can be installed;\n",
            "   â”œâ”€ \u001b[32mpytorch 1.13.0\u001b[0m would require\n",
            "   â”‚  â””â”€ \u001b[32mpython >=3.9,<3.10.0a0 \u001b[0m, which can be installed;\n",
            "   â””â”€ \u001b[31mpytorch 1.13.0\u001b[0m would require\n",
            "      â””â”€ \u001b[31m__cuda\u001b[0m, which is missing on the system.\n",
            "\n",
            "Looking for: ['scipy', 'scikit-image']\n",
            "\n",
            "conda-forge/linux-64                                        Using cache\n",
            "conda-forge/noarch                                          Using cache\n",
            "\n",
            "Pinned packages:\n",
            "  - python 3.11.*\n",
            "  - python 3.11.*\n",
            "  - python_abi 3.11.* *cp311*\n",
            "  - cuda-version 12.*\n",
            "\n",
            "\n",
            "Transaction\n",
            "\n",
            "  Prefix: /usr/local\n",
            "\n",
            "  All requested packages already installed\n",
            "\n",
            "\u001b[?25l\u001b[2K\u001b[0G\u001b[?25h\n",
            "Looking for: ['diffusers', 'transformers', 'ftfy', 'accelerate']\n",
            "\n",
            "conda-forge/linux-64                                        Using cache\n",
            "conda-forge/noarch                                          Using cache\n",
            "\n",
            "Pinned packages:\n",
            "  - python 3.11.*\n",
            "  - python 3.11.*\n",
            "  - python_abi 3.11.* *cp311*\n",
            "  - cuda-version 12.*\n",
            "\n",
            "\n",
            "Transaction\n",
            "\n",
            "  Prefix: /usr/local\n",
            "\n",
            "  All requested packages already installed\n",
            "\n",
            "\u001b[?25l\u001b[2K\u001b[0G\u001b[?25h/usr/bin/python3.10: No module named pip\n",
            "/usr/bin/python3.10: No module named pip\n",
            "/usr/bin/python3.10: No module named pip\n",
            "/usr/bin/python3.10: No module named pip\n",
            "Cloning into 'diffusers'...\n",
            "remote: Enumerating objects: 25615, done.\u001b[K\n",
            "remote: Counting objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 25615 (delta 0), reused 0 (delta 0), pack-reused 25614 (from 1)\u001b[K\n",
            "Receiving objects: 100% (25615/25615), 19.04 MiB | 16.69 MiB/s, done.\n",
            "Resolving deltas: 100% (17683/17683), done.\n",
            "/content/diffusers/diffusers\n",
            "Note: switching to '9a37409663a53f775fa380db332d37d7ea75c915'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by switching back to a branch.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -c with the switch command. Example:\n",
            "\n",
            "  git switch -c <new-branch-name>\n",
            "\n",
            "Or undo this operation with:\n",
            "\n",
            "  git switch -\n",
            "\n",
            "Turn off this advice by setting config variable advice.detachedHead to false\n",
            "\n",
            "HEAD is now at 9a374096 Update copyright year\n",
            "/usr/bin/python3.10: No module named pip\n",
            "/usr/bin/python3.10: No module named pip\n",
            "/usr/bin/python3.10: No module named pip\n",
            "\n",
            "Looking for: ['iopath']\n",
            "\n",
            "conda-forge/linux-64                                        Using cache\n",
            "conda-forge/noarch                                          Using cache\n",
            "\u001b[?25l\u001b[2K\u001b[0G[+] 0.0s\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.1s\n",
            "iopath/linux-64  â£¾  \n",
            "iopath/noarch    â£¾  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.2s\n",
            "iopath/linux-64  â£¾  \n",
            "iopath/noarch    â£¾  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Giopath/noarch                                                 No change\n",
            "iopath/linux-64                                               No change\n",
            "\u001b[?25h\n",
            "Pinned packages:\n",
            "  - python 3.11.*\n",
            "  - python 3.11.*\n",
            "  - python_abi 3.11.* *cp311*\n",
            "  - cuda-version 12.*\n",
            "\n",
            "\n",
            "Transaction\n",
            "\n",
            "  Prefix: /usr/local\n",
            "\n",
            "  All requested packages already installed\n",
            "\n",
            "\u001b[?25l\u001b[2K\u001b[0G\u001b[?25h\n",
            "Looking for: ['nvidiacub']\n",
            "\n",
            "conda-forge/linux-64                                        Using cache\n",
            "conda-forge/noarch                                          Using cache\n",
            "\u001b[?25l\u001b[2K\u001b[0G[+] 0.0s\n",
            "bottler/linux-64  â£¾  \n",
            "bottler/noarch    â£¾  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.1s\n",
            "bottler/linux-64  â£¾  \n",
            "bottler/noarch    â£¾  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gbottler/noarch                                                No change\n",
            "bottler/linux-64                                              No change\n",
            "\u001b[?25h\n",
            "Pinned packages:\n",
            "  - python 3.11.*\n",
            "  - python 3.11.*\n",
            "  - python_abi 3.11.* *cp311*\n",
            "  - cuda-version 12.*\n",
            "\n",
            "\n",
            "Transaction\n",
            "\n",
            "  Prefix: /usr/local\n",
            "\n",
            "  All requested packages already installed\n",
            "\n",
            "\u001b[?25l\u001b[2K\u001b[0G\u001b[?25h\n",
            "Looking for: ['pytorch3d']\n",
            "\n",
            "conda-forge/linux-64                                        Using cache\n",
            "conda-forge/noarch                                          Using cache\n",
            "\u001b[?25l\u001b[2K\u001b[0G[+] 0.0s\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.1s\n",
            "pytorch3d/linux-64  â£¾  \n",
            "pytorch3d/noarch    â£¾  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gpytorch3d/linux-64                                            No change\n",
            "pytorch3d/noarch                                              No change\n",
            "\u001b[?25h\n",
            "Pinned packages:\n",
            "  - python 3.11.*\n",
            "  - python 3.11.*\n",
            "  - python_abi 3.11.* *cp311*\n",
            "  - cuda-version 12.*\n",
            "\n",
            "\n",
            "Transaction\n",
            "\n",
            "  Prefix: /usr/local\n",
            "\n",
            "  All requested packages already installed\n",
            "\n",
            "\u001b[?25l\u001b[2K\u001b[0G\u001b[?25h/usr/bin/python3.10: No module named pip\n",
            "\n",
            "Looking for: ['openexr-python', 'openexr']\n",
            "\n",
            "conda-forge/linux-64                                        Using cache\n",
            "conda-forge/noarch                                          Using cache\n",
            "\n",
            "Pinned packages:\n",
            "  - python 3.11.*\n",
            "  - python 3.11.*\n",
            "  - python_abi 3.11.* *cp311*\n",
            "  - cuda-version 12.*\n",
            "\n",
            "\n",
            "Transaction\n",
            "\n",
            "  Prefix: /usr/local\n",
            "\n",
            "  All requested packages already installed\n",
            "\n",
            "\u001b[?25l\u001b[2K\u001b[0G\u001b[?25h\n",
            "Looking for: ['pyshtools']\n",
            "\n",
            "conda-forge/linux-64                                        Using cache\n",
            "conda-forge/noarch                                          Using cache\n",
            "\n",
            "Pinned packages:\n",
            "  - python 3.11.*\n",
            "  - python 3.11.*\n",
            "  - python_abi 3.11.* *cp311*\n",
            "  - cuda-version 12.*\n",
            "\n",
            "\n",
            "Transaction\n",
            "\n",
            "  Prefix: /usr/local\n",
            "\n",
            "  All requested packages already installed\n",
            "\n",
            "\u001b[?25l\u001b[2K\u001b[0G\u001b[?25h/usr/bin/python3.10: No module named pip\n",
            "/usr/bin/python3.10: No module named pip\n",
            "/usr/bin/python3.10: No module named pip\n"
          ]
        }
      ],
      "source": [
        "!mamba create -n icontrol3d python=3.10\n",
        "!mamba activate icontrol3d\n",
        "!mamba install pytorch=1.13.0 torchvision torchaudio pytorch-cuda=11.6 -c pytorch -c nvidia\n",
        "!mamba install scipy scikit-image\n",
        "!mamba install -c conda-forge diffusers transformers ftfy accelerate\n",
        "!python3.10 -m pip install opencv-python\n",
        "!python3.10 -m pip install -U gradio\n",
        "!python3.10 -m pip install pytorch-lightning==1.7.7 einops==0.4.1 omegaconf==2.2.3\n",
        "!python3.10 -m pip install timm\n",
        "\n",
        "# Install diffusers\n",
        "!git clone https://github.com/takuma104/diffusers.git\n",
        "%cd diffusers\n",
        "!git checkout 9a37409663a53f775fa380db332d37d7ea75c915\n",
        "!python3.10 -m pip install .\n",
        "\n",
        "# Update transformers and huggingface_hub\n",
        "!python3.10 -m pip install git+https://github.com/huggingface/transformers\n",
        "!python3.10 -m pip install -U huggingface_hub\n",
        "\n",
        "# Pytorch3D\n",
        "!mamba install -c iopath iopath\n",
        "!mamba install -c bottler nvidiacub\n",
        "!mamba install pytorch3d -c pytorch3d\n",
        "\n",
        "# skylibs\n",
        "!python3.10 -m pip install --upgrade skylibs\n",
        "!mamba install -c conda-forge openexr-python openexr\n",
        "!mamba install -c conda-forge pyshtools\n",
        "\n",
        "# Grounded-Segment-Anything\n",
        "!python3.10 -m pip install -e segment_anything\n",
        "!python3.10 -m pip install -e GroundingDINO\n",
        "!python3.10 -m pip install opencv-python pycocotools matplotlib onnxruntime onnx ipykernel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/lllyasviel/ControlNet.git\n",
        "%cd ControlNet\n",
        "!wget https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5/resolve/main/v1-5-pruned.ckpt -P models\n",
        "!wget https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_openpose.pth -P models\n",
        "!wget https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_depth.pth -P models\n",
        "!wget https://huggingface.co/botp/Anything-Preservation/resolve/1513b19f87140ded29ecb9da3240d94149a262f6/anything-v3-full.safetensors -P models\n",
        "!wget https://huggingface.co/toyxyz/Control_any3/resolve/main/control_any3_openpose.pth -P models\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dhsm6PLcMxY",
        "outputId": "5669b334-447c-4a30-fe55-64511d3b02e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ControlNet'...\n",
            "remote: Enumerating objects: 1356, done.\u001b[K\n",
            "remote: Total 1356 (delta 0), reused 0 (delta 0), pack-reused 1356 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1356/1356), 122.40 MiB | 35.46 MiB/s, done.\n",
            "Resolving deltas: 100% (599/599), done.\n",
            "/content/diffusers/diffusers/ControlNet\n",
            "--2025-03-17 11:08:48--  https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5/resolve/main/v1-5-pruned.ckpt\n",
            "Resolving huggingface.co (huggingface.co)... 18.172.134.24, 18.172.134.88, 18.172.134.4, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.172.134.24|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.hf.co/repos/66/6f/666f465fa70158515404e8de2c6bc6fe2f90c46f9296293aa14daededeb32c52/e1441589a6f3c5a53f5f54d0975a18a7feb7cdf0b0dee276dfc3331ae376a053?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27v1-5-pruned.ckpt%3B+filename%3D%22v1-5-pruned.ckpt%22%3B&Expires=1742213328&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MjIxMzMyOH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy82Ni82Zi82NjZmNDY1ZmE3MDE1ODUxNTQwNGU4ZGUyYzZiYzZmZTJmOTBjNDZmOTI5NjI5M2FhMTRkYWVkZWRlYjMyYzUyL2UxNDQxNTg5YTZmM2M1YTUzZjVmNTRkMDk3NWExOGE3ZmViN2NkZjBiMGRlZTI3NmRmYzMzMzFhZTM3NmEwNTM%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=qUamF4Ik7hephOEPBXjezvb4LiCaOkEVjOQ2YrLDy9EGtMfwJ0nJLjjSmf7Dzt-LbJGZUqaDdrary1SmTGimYFLt842sbM%7EyPD5caboPvnlDODNurkqTbes5afdua6LXwoEDhXgNaxM34WVIkhdc3jyXjO9daSwJTCsGzV5lWHrk82rjdRDCDKe2knrJLCSi-YmPbBSPGpbeCx6OO3REUZuIkFLwUnJBsj0QmgNZWqZ0O227DVLDJP8PNOIvlcKt0DDfY9M0OCWvqJ19k%7EUYZLH0ksz1GDwmjk0i9Es4UmsFhJ6g0JXbIXkDgwkZclttUeDZ6kbmL%7Ep8AlO2-iKBiQ__&Key-Pair-Id=K3RPWS32NSSJCE [following]\n",
            "--2025-03-17 11:08:48--  https://cdn-lfs.hf.co/repos/66/6f/666f465fa70158515404e8de2c6bc6fe2f90c46f9296293aa14daededeb32c52/e1441589a6f3c5a53f5f54d0975a18a7feb7cdf0b0dee276dfc3331ae376a053?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27v1-5-pruned.ckpt%3B+filename%3D%22v1-5-pruned.ckpt%22%3B&Expires=1742213328&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MjIxMzMyOH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy82Ni82Zi82NjZmNDY1ZmE3MDE1ODUxNTQwNGU4ZGUyYzZiYzZmZTJmOTBjNDZmOTI5NjI5M2FhMTRkYWVkZWRlYjMyYzUyL2UxNDQxNTg5YTZmM2M1YTUzZjVmNTRkMDk3NWExOGE3ZmViN2NkZjBiMGRlZTI3NmRmYzMzMzFhZTM3NmEwNTM%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=qUamF4Ik7hephOEPBXjezvb4LiCaOkEVjOQ2YrLDy9EGtMfwJ0nJLjjSmf7Dzt-LbJGZUqaDdrary1SmTGimYFLt842sbM%7EyPD5caboPvnlDODNurkqTbes5afdua6LXwoEDhXgNaxM34WVIkhdc3jyXjO9daSwJTCsGzV5lWHrk82rjdRDCDKe2knrJLCSi-YmPbBSPGpbeCx6OO3REUZuIkFLwUnJBsj0QmgNZWqZ0O227DVLDJP8PNOIvlcKt0DDfY9M0OCWvqJ19k%7EUYZLH0ksz1GDwmjk0i9Es4UmsFhJ6g0JXbIXkDgwkZclttUeDZ6kbmL%7Ep8AlO2-iKBiQ__&Key-Pair-Id=K3RPWS32NSSJCE\n",
            "Resolving cdn-lfs.hf.co (cdn-lfs.hf.co)... 3.167.152.106, 3.167.152.119, 3.167.152.12, ...\n",
            "Connecting to cdn-lfs.hf.co (cdn-lfs.hf.co)|3.167.152.106|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7703807346 (7.2G) [binary/octet-stream]\n",
            "Saving to: â€˜models/v1-5-pruned.ckptâ€™\n",
            "\n",
            "v1-5-pruned.ckpt    100%[===================>]   7.17G   220MB/s    in 60s     \n",
            "\n",
            "2025-03-17 11:09:49 (122 MB/s) - â€˜models/v1-5-pruned.ckptâ€™ saved [7703807346/7703807346]\n",
            "\n",
            "--2025-03-17 11:09:49--  https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_openpose.pth\n",
            "Resolving huggingface.co (huggingface.co)... 18.172.134.124, 18.172.134.88, 18.172.134.4, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.172.134.124|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.hf.co/repos/f6/65/f665e553e8fd16131981a96c629e785939f08080bb3cbf8d2b7f5ddbd9e6100a/d19ffffeeaff6d9feb2204b234c3e1b9aec039ab3e63fca07f4fe5646f2ef591?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27control_sd15_openpose.pth%3B+filename%3D%22control_sd15_openpose.pth%22%3B&Expires=1742213389&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MjIxMzM4OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy9mNi82NS9mNjY1ZTU1M2U4ZmQxNjEzMTk4MWE5NmM2MjllNzg1OTM5ZjA4MDgwYmIzY2JmOGQyYjdmNWRkYmQ5ZTYxMDBhL2QxOWZmZmZlZWFmZjZkOWZlYjIyMDRiMjM0YzNlMWI5YWVjMDM5YWIzZTYzZmNhMDdmNGZlNTY0NmYyZWY1OTE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=Mcu2w30h2qduneXPnfPc7d8SYKyavRVAB5naHl0weaPBTgNfiTmoyv8gbE%7EoBlllPQUr8n6QxhG4xNyYZIc7JEzvOf1kQnOm7LRD89ELF-vIUieVPrxpoCb0Gj19imy4duHdlW7htWgOWFUHHaVPgySQTI45s32deG%7EiOE8tmKuraS6PaQhSvNY18QgrN%7EfvQfqTt1R4qjn5oswRg6gR%7EJ51Ko4HbApI-a0u41CsOXAM%7EQNqmg0P90K-L2GBnEKyOMAUS-P6Qsxt5SpyeqnOm7ovo9W6PfgBqhU3oSXJUxhV%7EufGgIZTkjBuONuZF7y4l47MioJ-HR7lqf7KUScK3g__&Key-Pair-Id=K3RPWS32NSSJCE [following]\n",
            "--2025-03-17 11:09:49--  https://cdn-lfs.hf.co/repos/f6/65/f665e553e8fd16131981a96c629e785939f08080bb3cbf8d2b7f5ddbd9e6100a/d19ffffeeaff6d9feb2204b234c3e1b9aec039ab3e63fca07f4fe5646f2ef591?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27control_sd15_openpose.pth%3B+filename%3D%22control_sd15_openpose.pth%22%3B&Expires=1742213389&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MjIxMzM4OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy9mNi82NS9mNjY1ZTU1M2U4ZmQxNjEzMTk4MWE5NmM2MjllNzg1OTM5ZjA4MDgwYmIzY2JmOGQyYjdmNWRkYmQ5ZTYxMDBhL2QxOWZmZmZlZWFmZjZkOWZlYjIyMDRiMjM0YzNlMWI5YWVjMDM5YWIzZTYzZmNhMDdmNGZlNTY0NmYyZWY1OTE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=Mcu2w30h2qduneXPnfPc7d8SYKyavRVAB5naHl0weaPBTgNfiTmoyv8gbE%7EoBlllPQUr8n6QxhG4xNyYZIc7JEzvOf1kQnOm7LRD89ELF-vIUieVPrxpoCb0Gj19imy4duHdlW7htWgOWFUHHaVPgySQTI45s32deG%7EiOE8tmKuraS6PaQhSvNY18QgrN%7EfvQfqTt1R4qjn5oswRg6gR%7EJ51Ko4HbApI-a0u41CsOXAM%7EQNqmg0P90K-L2GBnEKyOMAUS-P6Qsxt5SpyeqnOm7ovo9W6PfgBqhU3oSXJUxhV%7EufGgIZTkjBuONuZF7y4l47MioJ-HR7lqf7KUScK3g__&Key-Pair-Id=K3RPWS32NSSJCE\n",
            "Resolving cdn-lfs.hf.co (cdn-lfs.hf.co)... 3.167.152.106, 3.167.152.37, 3.167.152.119, ...\n",
            "Connecting to cdn-lfs.hf.co (cdn-lfs.hf.co)|3.167.152.106|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5710751843 (5.3G) [binary/octet-stream]\n",
            "Saving to: â€˜models/control_sd15_openpose.pthâ€™\n",
            "\n",
            "control_sd15_openpo 100%[===================>]   5.32G   202MB/s    in 38s     \n",
            "\n",
            "2025-03-17 11:10:27 (143 MB/s) - â€˜models/control_sd15_openpose.pthâ€™ saved [5710751843/5710751843]\n",
            "\n",
            "--2025-03-17 11:10:27--  https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_depth.pth\n",
            "Resolving huggingface.co (huggingface.co)... 18.172.134.4, 18.172.134.24, 18.172.134.88, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.172.134.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.hf.co/repos/f6/65/f665e553e8fd16131981a96c629e785939f08080bb3cbf8d2b7f5ddbd9e6100a/726cd0b472c4b5c0341b01afcb7fdc4a7b4ab7c37fe797fd394c9805cbef60bf?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27control_sd15_depth.pth%3B+filename%3D%22control_sd15_depth.pth%22%3B&Expires=1742213428&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MjIxMzQyOH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy9mNi82NS9mNjY1ZTU1M2U4ZmQxNjEzMTk4MWE5NmM2MjllNzg1OTM5ZjA4MDgwYmIzY2JmOGQyYjdmNWRkYmQ5ZTYxMDBhLzcyNmNkMGI0NzJjNGI1YzAzNDFiMDFhZmNiN2ZkYzRhN2I0YWI3YzM3ZmU3OTdmZDM5NGM5ODA1Y2JlZjYwYmY%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=DXlLPTAvX%7E7sW%7ECH7olQiEqcGv9fYCUoByjH%7E5yuT9Ynt%7EOuGjCKsWdBADHLyPSMig7kqqiCZr7XyRjyHYHdf1sZIbxfIuGaS-Sf%7E5veNMhPs5NtfaV01TtCVOZ9EgxdED4oRJ1xGYH6%7EhYEcHn%7EEmmCdhoI3O2h4xXiRcrUOOYS6uNKjlC7eZr-nOe-s3hiRY3IRS4nPiBCQ7fu5BXkbQKBTTf5epmIfsSXhuyRB-ooaZP34dFyU5syY3puv9yWF753COaYBaSMh4M7wSSzkA-uIr3lmYxy8D8wf00brnHA11XtY1Tvw6K9GgJir%7Eua9Kh2XmlW2fVLrFeS4ggR8A__&Key-Pair-Id=K3RPWS32NSSJCE [following]\n",
            "--2025-03-17 11:10:28--  https://cdn-lfs.hf.co/repos/f6/65/f665e553e8fd16131981a96c629e785939f08080bb3cbf8d2b7f5ddbd9e6100a/726cd0b472c4b5c0341b01afcb7fdc4a7b4ab7c37fe797fd394c9805cbef60bf?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27control_sd15_depth.pth%3B+filename%3D%22control_sd15_depth.pth%22%3B&Expires=1742213428&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MjIxMzQyOH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy9mNi82NS9mNjY1ZTU1M2U4ZmQxNjEzMTk4MWE5NmM2MjllNzg1OTM5ZjA4MDgwYmIzY2JmOGQyYjdmNWRkYmQ5ZTYxMDBhLzcyNmNkMGI0NzJjNGI1YzAzNDFiMDFhZmNiN2ZkYzRhN2I0YWI3YzM3ZmU3OTdmZDM5NGM5ODA1Y2JlZjYwYmY%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=DXlLPTAvX%7E7sW%7ECH7olQiEqcGv9fYCUoByjH%7E5yuT9Ynt%7EOuGjCKsWdBADHLyPSMig7kqqiCZr7XyRjyHYHdf1sZIbxfIuGaS-Sf%7E5veNMhPs5NtfaV01TtCVOZ9EgxdED4oRJ1xGYH6%7EhYEcHn%7EEmmCdhoI3O2h4xXiRcrUOOYS6uNKjlC7eZr-nOe-s3hiRY3IRS4nPiBCQ7fu5BXkbQKBTTf5epmIfsSXhuyRB-ooaZP34dFyU5syY3puv9yWF753COaYBaSMh4M7wSSzkA-uIr3lmYxy8D8wf00brnHA11XtY1Tvw6K9GgJir%7Eua9Kh2XmlW2fVLrFeS4ggR8A__&Key-Pair-Id=K3RPWS32NSSJCE\n",
            "Resolving cdn-lfs.hf.co (cdn-lfs.hf.co)... 3.167.152.119, 3.167.152.12, 3.167.152.106, ...\n",
            "Connecting to cdn-lfs.hf.co (cdn-lfs.hf.co)|3.167.152.119|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5710753329 (5.3G) [binary/octet-stream]\n",
            "Saving to: â€˜models/control_sd15_depth.pthâ€™\n",
            "\n",
            "control_sd15_depth. 100%[===================>]   5.32G   211MB/s    in 41s     \n",
            "\n",
            "2025-03-17 11:11:09 (133 MB/s) - â€˜models/control_sd15_depth.pthâ€™ saved [5710753329/5710753329]\n",
            "\n",
            "--2025-03-17 11:11:09--  https://huggingface.co/botp/Anything-Preservation/resolve/1513b19f87140ded29ecb9da3240d94149a262f6/anything-v3-full.safetensors\n",
            "Resolving huggingface.co (huggingface.co)... 18.172.134.88, 18.172.134.4, 18.172.134.124, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.172.134.88|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.hf.co/repos/f4/4f/f44f4af87249882a4e4f0cf65ceee05192dab0581face06a7f97faf03f03be70/abcaf14e5acb8023c79c3901f8ffc04eb3c646d7793f3b36a439bf09e32868cd?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27anything-v3-full.safetensors%3B+filename%3D%22anything-v3-full.safetensors%22%3B&Expires=1742213469&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MjIxMzQ2OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy9mNC80Zi9mNDRmNGFmODcyNDk4ODJhNGU0ZjBjZjY1Y2VlZTA1MTkyZGFiMDU4MWZhY2UwNmE3Zjk3ZmFmMDNmMDNiZTcwL2FiY2FmMTRlNWFjYjgwMjNjNzljMzkwMWY4ZmZjMDRlYjNjNjQ2ZDc3OTNmM2IzNmE0MzliZjA5ZTMyODY4Y2Q%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=YbpCdrWUVk6C4M0S%7Ebk9l0H5Ll17Cy4iOmsoh0RREgpKNKNaKm1Uyrdsh-Jw3f2bOG27OWv1zg6UzIzLxZINC8bmMAn7bUewuf-c0SMvjL%7E6j4-iGipbtkeJzsC4fG7IjuWbEZIlFZC0OKOYqIW%7EmKatuLUcLF-s5wSFoX%7EjaPjt%7E4rRQPC-TT78-kd-TP1rXr2OwqAvH7jY%7ESnyovYMr0TO5J17Gz9HUwuC5ZdRyJVarGEmP8iKgdD-TFpEJP6VJMCHXgbC0V7sa5GDlmQJzKeNMFMjs7cTIQvJX6-eKkRIrD9ZUU3W8w71pg0F2fb6RFdOudjTFFviwqnFqQ9qQw__&Key-Pair-Id=K3RPWS32NSSJCE [following]\n",
            "--2025-03-17 11:11:09--  https://cdn-lfs.hf.co/repos/f4/4f/f44f4af87249882a4e4f0cf65ceee05192dab0581face06a7f97faf03f03be70/abcaf14e5acb8023c79c3901f8ffc04eb3c646d7793f3b36a439bf09e32868cd?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27anything-v3-full.safetensors%3B+filename%3D%22anything-v3-full.safetensors%22%3B&Expires=1742213469&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MjIxMzQ2OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy9mNC80Zi9mNDRmNGFmODcyNDk4ODJhNGU0ZjBjZjY1Y2VlZTA1MTkyZGFiMDU4MWZhY2UwNmE3Zjk3ZmFmMDNmMDNiZTcwL2FiY2FmMTRlNWFjYjgwMjNjNzljMzkwMWY4ZmZjMDRlYjNjNjQ2ZDc3OTNmM2IzNmE0MzliZjA5ZTMyODY4Y2Q%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=YbpCdrWUVk6C4M0S%7Ebk9l0H5Ll17Cy4iOmsoh0RREgpKNKNaKm1Uyrdsh-Jw3f2bOG27OWv1zg6UzIzLxZINC8bmMAn7bUewuf-c0SMvjL%7E6j4-iGipbtkeJzsC4fG7IjuWbEZIlFZC0OKOYqIW%7EmKatuLUcLF-s5wSFoX%7EjaPjt%7E4rRQPC-TT78-kd-TP1rXr2OwqAvH7jY%7ESnyovYMr0TO5J17Gz9HUwuC5ZdRyJVarGEmP8iKgdD-TFpEJP6VJMCHXgbC0V7sa5GDlmQJzKeNMFMjs7cTIQvJX6-eKkRIrD9ZUU3W8w71pg0F2fb6RFdOudjTFFviwqnFqQ9qQw__&Key-Pair-Id=K3RPWS32NSSJCE\n",
            "Resolving cdn-lfs.hf.co (cdn-lfs.hf.co)... 3.167.152.106, 3.167.152.12, 3.167.152.37, ...\n",
            "Connecting to cdn-lfs.hf.co (cdn-lfs.hf.co)|3.167.152.106|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7703274858 (7.2G) [binary/octet-stream]\n",
            "Saving to: â€˜models/anything-v3-full.safetensorsâ€™\n",
            "\n",
            "anything-v3-full.sa 100%[===================>]   7.17G   142MB/s    in 48s     \n",
            "\n",
            "2025-03-17 11:11:57 (154 MB/s) - â€˜models/anything-v3-full.safetensorsâ€™ saved [7703274858/7703274858]\n",
            "\n",
            "--2025-03-17 11:11:57--  https://huggingface.co/toyxyz/Control_any3/resolve/main/control_any3_openpose.pth\n",
            "Resolving huggingface.co (huggingface.co)... 18.172.134.124, 18.172.134.4, 18.172.134.88, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.172.134.124|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.hf.co/repos/da/96/da969db9f00a6eb55f0917ba872ae986f9854a9df66cc68f18e224a132887481/99ea3e8ee415dad0c0fe949df92ecd6bce5d8467dfebc9d9656d8f138070e9c5?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27control_any3_openpose.pth%3B+filename%3D%22control_any3_openpose.pth%22%3B&Expires=1742213517&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MjIxMzUxN319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy9kYS85Ni9kYTk2OWRiOWYwMGE2ZWI1NWYwOTE3YmE4NzJhZTk4NmY5ODU0YTlkZjY2Y2M2OGYxOGUyMjRhMTMyODg3NDgxLzk5ZWEzZThlZTQxNWRhZDBjMGZlOTQ5ZGY5MmVjZDZiY2U1ZDg0NjdkZmViYzlkOTY1NmQ4ZjEzODA3MGU5YzU%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=iNPvrk3oVKOGNYrkUvn-xqKpS%7ELKpCKezcJ8nT6n9-p446fDk7LNjTg2H7aCWuDVlLnDGsWcKIinO85Y6I9mj54OSaydjlIOp8powfLmgI8CuWxPDosynZ3R2WD7k8VNjca7Bz6cJU2kavk6cBKEmbbkjBt8It6jVsqgKU3d3r%7E2hWAnVqJJ6jGk5EDGzQomLw3S%7EBMrRCQuoDVd2TQrGbRJgkAoaBImHgJJBci09PO1l7Ttx3aOFqSHjlkSm1pKS2jGfQ-HxVmE8ZelRnA5B9eL%7EZp3KDDCL-tiV5jV8ignbA5TQAC0nAQ4aqZQygcty3EIHKHNg6ijFuqPFo1D9g__&Key-Pair-Id=K3RPWS32NSSJCE [following]\n",
            "--2025-03-17 11:11:57--  https://cdn-lfs.hf.co/repos/da/96/da969db9f00a6eb55f0917ba872ae986f9854a9df66cc68f18e224a132887481/99ea3e8ee415dad0c0fe949df92ecd6bce5d8467dfebc9d9656d8f138070e9c5?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27control_any3_openpose.pth%3B+filename%3D%22control_any3_openpose.pth%22%3B&Expires=1742213517&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MjIxMzUxN319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy9kYS85Ni9kYTk2OWRiOWYwMGE2ZWI1NWYwOTE3YmE4NzJhZTk4NmY5ODU0YTlkZjY2Y2M2OGYxOGUyMjRhMTMyODg3NDgxLzk5ZWEzZThlZTQxNWRhZDBjMGZlOTQ5ZGY5MmVjZDZiY2U1ZDg0NjdkZmViYzlkOTY1NmQ4ZjEzODA3MGU5YzU%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=iNPvrk3oVKOGNYrkUvn-xqKpS%7ELKpCKezcJ8nT6n9-p446fDk7LNjTg2H7aCWuDVlLnDGsWcKIinO85Y6I9mj54OSaydjlIOp8powfLmgI8CuWxPDosynZ3R2WD7k8VNjca7Bz6cJU2kavk6cBKEmbbkjBt8It6jVsqgKU3d3r%7E2hWAnVqJJ6jGk5EDGzQomLw3S%7EBMrRCQuoDVd2TQrGbRJgkAoaBImHgJJBci09PO1l7Ttx3aOFqSHjlkSm1pKS2jGfQ-HxVmE8ZelRnA5B9eL%7EZp3KDDCL-tiV5jV8ignbA5TQAC0nAQ4aqZQygcty3EIHKHNg6ijFuqPFo1D9g__&Key-Pair-Id=K3RPWS32NSSJCE\n",
            "Resolving cdn-lfs.hf.co (cdn-lfs.hf.co)... 3.167.152.12, 3.167.152.119, 3.167.152.106, ...\n",
            "Connecting to cdn-lfs.hf.co (cdn-lfs.hf.co)|3.167.152.12|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5710621397 (5.3G) [binary/octet-stream]\n",
            "Saving to: â€˜models/control_any3_openpose.pthâ€™\n",
            "\n",
            "control_any3_openpo 100%[===================>]   5.32G  94.8MB/s    in 52s     \n",
            "\n",
            "2025-03-17 11:12:49 (106 MB/s) - â€˜models/control_any3_openpose.pthâ€™ saved [5710621397/5710621397]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops open_clip_torch omegaconf pytorch_lightning\n",
        "# change /content/diffusers/ControlNet/cldm/model.py line 18 to state_dict = get_state_dict(torch.load(ckpt_path, map_location=torch.device(location),weights_only=False))\n",
        "!python tool_transfer_control.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnEiceUAcPAc",
        "outputId": "a03fc833-5f8b-4778-eeb2-55537d552925"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/site-packages (0.8.1)\n",
            "Requirement already satisfied: open_clip_torch in /usr/local/lib/python3.11/site-packages (2.31.0)\n",
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.11/site-packages (2.3.0)\n",
            "Requirement already satisfied: pytorch_lightning in /usr/local/lib/python3.11/site-packages (2.5.0.post0)\n",
            "Requirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.11/site-packages (from open_clip_torch) (2.6.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/site-packages (from open_clip_torch) (0.21.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/site-packages (from open_clip_torch) (2024.11.6)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.11/site-packages (from open_clip_torch) (6.3.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/site-packages (from open_clip_torch) (4.67.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/site-packages (from open_clip_torch) (0.29.3)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/site-packages (from open_clip_torch) (0.5.3)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.11/site-packages (from open_clip_torch) (1.0.15)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/site-packages (from omegaconf) (4.9.3)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.11/site-packages (from omegaconf) (6.0.2)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.11/site-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (2024.12.0)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.11/site-packages (from pytorch_lightning) (1.6.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/site-packages (from pytorch_lightning) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/site-packages (from pytorch_lightning) (4.12.2)\n",
            "Requirement already satisfied: lightning-utilities>=0.10.0 in /usr/local/lib/python3.11/site-packages (from pytorch_lightning) (0.14.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/site-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (3.11.13)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/site-packages (from lightning-utilities>=0.10.0->pytorch_lightning) (65.6.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/site-packages (from torch>=1.9.0->open_clip_torch) (3.18.0)\n",
            "Requirement already satisfied: sympy!=1.13.2,>=1.13.1 in /usr/local/lib/python3.11/site-packages (from torch>=1.9.0->open_clip_torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/site-packages (from torch>=1.9.0->open_clip_torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/site-packages (from torch>=1.9.0->open_clip_torch) (3.1.6)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/site-packages (from torchmetrics>=0.7.0->pytorch_lightning) (2.2.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/site-packages (from ftfy->open_clip_torch) (0.2.13)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/site-packages (from huggingface-hub->open_clip_torch) (2.32.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/site-packages (from torchvision->open_clip_torch) (11.1.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.18.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/site-packages (from sympy!=1.13.2,>=1.13.1->torch>=1.9.0->open_clip_torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/site-packages (from jinja2->torch>=1.9.0->open_clip_torch) (2.1.5)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests->huggingface-hub->open_clip_torch) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests->huggingface-hub->open_clip_torch) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests->huggingface-hub->open_clip_torch) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests->huggingface-hub->open_clip_torch) (2025.1.31)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/diffusers/diffusers/ControlNet/tool_transfer_control.py\", line 17, in <module>\n",
            "    from share import *\n",
            "  File \"/content/diffusers/diffusers/ControlNet/share.py\", line 2, in <module>\n",
            "    from cldm.hack import disable_verbosity, enable_sliced_attention\n",
            "  File \"/content/diffusers/diffusers/ControlNet/cldm/hack.py\", line 4, in <module>\n",
            "    import ldm.modules.encoders.modules\n",
            "  File \"/content/diffusers/diffusers/ControlNet/ldm/modules/encoders/modules.py\", line 5, in <module>\n",
            "    from transformers import T5Tokenizer, T5EncoderModel, CLIPTokenizer, CLIPTextModel\n",
            "  File \"<frozen importlib._bootstrap>\", line 1229, in _handle_fromlist\n",
            "  File \"/usr/local/lib/python3.11/site-packages/transformers/utils/import_utils.py\", line 1083, in __getattr__\n",
            "    module = self._get_module(self._class_to_module[name])\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/site-packages/transformers/utils/import_utils.py\", line 1093, in _get_module\n",
            "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/site-packages/transformers/models/__init__.py\", line 19, in <module>\n",
            "    from . import (\n",
            "  File \"/usr/local/lib/python3.11/site-packages/transformers/models/mt5/__init__.py\", line 40, in <module>\n",
            "    from ..t5.tokenization_t5_fast import T5TokenizerFast\n",
            "  File \"/usr/local/lib/python3.11/site-packages/transformers/models/t5/tokenization_t5_fast.py\", line 24, in <module>\n",
            "    from ...tokenization_utils_fast import PreTrainedTokenizerFast\n",
            "  File \"/usr/local/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py\", line 25, in <module>\n",
            "    import tokenizers.pre_tokenizers as pre_tokenizers_fast\n",
            "  File \"/usr/local/lib/python3.11/site-packages/tokenizers/__init__.py\", line 80, in <module>\n",
            "    from .tokenizers import (\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ..\n",
        "!git clone https://github.com/haofanwang/ControlNet-for-Diffusers\n",
        "%cp ControlNet-for-Diffusers/pipeline_stable_diffusion_controlnet_inpaint.py src/diffusers/stable_diffusion\n",
        " #add from .pipeline_stable_diffusion_controlnet_inpaint import StableDiffusionControlNetInpaintPipeline to line 83 diffusers/src/diffusers/pipelines/stable_diffusion/__init__.py\n",
        " #add to StableDiffusionControlNetInpaintPipeline, to line 97  diffusers/src/diffusers/pipelines/__init__.py\n",
        " #add to StableDiffusionControlNetInpaintPipeline, to line 140  diffusers/src/diffusers/__init__.py\n",
        "\n",
        "#replace line 29 in /usr/local/lib/python3.11/site-packages/diffusers/utils/dynamic_modules_utils.py\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9lWmHiistg0",
        "outputId": "7bbcbf40-e3e7-4d68-fa6e-0cbefb177380"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/diffusers\n",
            "Cloning into 'ControlNet-for-Diffusers'...\n",
            "remote: Enumerating objects: 152, done.\u001b[K\n",
            "remote: Counting objects: 100% (145/145), done.\u001b[K\n",
            "remote: Compressing objects: 100% (101/101), done.\u001b[K\n",
            "remote: Total 152 (delta 72), reused 98 (delta 43), pack-reused 7 (from 1)\u001b[K\n",
            "Receiving objects: 100% (152/152), 1.15 MiB | 8.49 MiB/s, done.\n",
            "Resolving deltas: 100% (73/73), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.25.1\n",
        "#python ./scripts/convert_controlnet_to_diffusers.py --checkpoint_path ./models/control_sd15_***.pth --dump_path ../controlnet_models/control_sd15_*** --device cpu\n",
        "!python ./scripts/convert_controlnet_to_diffusers.py --checkpoint_path ./ControlNet/models/control_sd15_openpose.pth --dump_path ../ControlNet/controlnet_models/control_sd15_openpose.pth --device cpu\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uu1qP2wzPUe1",
        "outputId": "abb22c94-f755-498c-e977-710fc2d43d92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers==4.25.1 in /usr/local/lib/python3.11/site-packages (4.25.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/site-packages (from transformers==4.25.1) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.11/site-packages (from transformers==4.25.1) (0.29.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/site-packages (from transformers==4.25.1) (2.2.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/site-packages (from transformers==4.25.1) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/site-packages (from transformers==4.25.1) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/site-packages (from transformers==4.25.1) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/site-packages (from transformers==4.25.1) (2.32.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.11/site-packages (from transformers==4.25.1) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/site-packages (from transformers==4.25.1) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.25.1) (2024.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.25.1) (4.12.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests->transformers==4.25.1) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests->transformers==4.25.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests->transformers==4.25.1) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests->transformers==4.25.1) (2025.1.31)\n",
            "global_step key not found in model\n",
            "/usr/local/lib/python3.11/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'visual_projection.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'logit_scale', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'text_projection.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias']\n",
            "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python app_controlnet_inpaint.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_rj-OMvU-Z2",
        "outputId": "6c9404a1-f89a-4322-aa0c-146b69c3f0da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python: can't open file '/content/diffusers/app_controlnet_inpaint.py': [Errno 2] No such file or directory\n"
          ]
        }
      ]
    }
  ]
}